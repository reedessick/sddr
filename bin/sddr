#!/usr/bin/env python

__doc__ = "an executable that computes things"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import os
import numpy as np

from argparse import ArgumentParser

### non-standard libraries
from sddr import utils

#-------------------------------------------------

parser = ArgumentParser(description=__doc__)

parser.add_argument('samples', nargs='+', type=str)

parser.add_argument('-v', '--verbose', default=False, action='store_true')
parser.add_argument('-V', '--Verbose', default=False, action='store_true')

parser.add_argument('--field', default=utils.DEFAULT_FIELD, type=str,
    help='the field selected for the 1D marginalized KDE. \
DEFAULT='+utils.DEFAULT_FIELD)
parser.add_argument('--exponentiate-field', default=False, action='store_true',
    help='exponentiate the values in --field and make a KDE of those.')
parser.add_argument('--exponentiate10-field', default=False, action='store_true',
    help='exponentiate the values in --field and make a KDE of those.')

parser.add_argument('--weight-field', default=None, type=str,
    help='if supplied, read values corresponding to this field and use a weights within KDE')
parser.add_argument('--exponentiate-weight-field', default=False, action='store_true',
    help='exponentiate the values in --weight-field if supplied and use those as the weights.')
parser.add_argument('--exponentiate10-weight-field', default=False, action='store_true',
    help='exponentiate the values in --weight-field if supplied and use those as the weights.')

parser.add_argument('--initial-burnin', default=utils.DEFAULT_INITIAL_BURNIN, type=int,
    help='force the code to skip the first --initial-burnin samples, and then proceed with --deltaLogP logic. \
DEFAULT=%d'%utils.DEFAULT_INITIAL_BURNIN)
parser.add_argument('--deltaLogP', default=utils.DEFAULT_DELTALOGP, type=float,
    help='used when stripping burn-in from hdf5 files')
parser.add_argument('--downsample', default=utils.DEFAULT_DOWNSAMPLE, type=int,
    help='only retain 1 out of every --downsample samples after stripping burn-in. Used when reading both file types')

parser.add_argument('--skip', default=[], type=str, action='append',
    help='skip the computation of this particular estimate. Note, we require an exact match of the name, so these options might be pretty long...')

parser.add_argument('--num-subsets', default=utils.DEFAULT_NUM_SUBSETS, type=int)

parser.add_argument('--prior-min', default=utils.DEFAULT_PRIOR_MIN, type=float)
parser.add_argument('--prior-max', default=utils.DEFAULT_PRIOR_MAX, type=float)
parser.add_argument('--evaluation-point', default=None, type=float,
    help='the value of log10NLTides_A0 used as the evaluation point')

parser.add_argument('--hist-bins', default=None, type=int)

parser.add_argument('--chist-bins', default=None, type=int)
parser.add_argument('--chist-fit-max', default=utils.DEFAULT_FIT_MAX, type=float,
    help='the maximum value used when fitting the cumulative histogram to a low-order polynomial. \
DEFAULT=%.f'%utils.DEFAULT_FIT_MAX)

parser.add_argument('--kde-b', default=utils.DEFAULT_B, type=float)

parser.add_argument('--kde-b-range', nargs=2, default=utils.DEFAULT_B_RANGE, type=float)
parser.add_argument('--kde-rtol', default=utils.DEFAULT_RTOL, type=float)

parser.add_argument('--kde-dlogl', default=utils.DEFAULT_DLOGL, type=float)
parser.add_argument('--kde-num-points', default=utils.DEFAULT_NUM_POINTS, type=int)
parser.add_argument('--kde-b-prior', default=utils.DEFAULT_B_PRIOR, type=str)

parser.add_argument('--kde-beta-method', default='marg', type=str,
    help='either "marg" or "max" to control how we choose the bandwidth for sampling error fit to beta-distribution. \
DEFAULT=marg')
parser.add_argument('--kde-num-quantiles', default=utils.DEFAULT_NUM_QUANTILES, type=int)

parser.add_argument('-o', '--output-dir', default='.', type=str)
parser.add_argument('-t', '--tag', default='', type=str)

args = parser.parse_args()

assert not (args.exponentiate_field and args.exponentiate10_field), 'please supply either --exponentiate-field or --exponentiate10-field, but not both'
assert not (args.exponentiate_weight_field and args.exponentiate10_weight_field), 'please supply either --exponentiate-weight-field or --exponentiate10-weight-field, but not both'

args.verbose |= args.Verbose

if not os.path.exists(args.output_dir):
    os.makedirs(args.output_dir)

if args.tag:
    args.tag = "_"+args.tag

logprior = -np.log(args.prior_max-args.prior_min)
if args.evaluation_point is None:
    args.evaluation_point = args.prior_min

#-------------------------------------------------

### read in samples
samples = utils.load(args.samples, field=args.field, deltaLogP=args.deltaLogP, downsample=args.downsample, initial_burnin=args.initial_burnin, verbose=args.verbose)
if args.exponentiate_field:
    if args.verbose:
        print('exponentiating samples')
    samples = np.exp(samples)
elif args.exponentiate10_field:
    if args.verbose:
        print('exponentiating10 samples')
    samples = 10**samples

if args.weight_field:
    weights = utils.load(args.samples, field=args.weight_field, deltaLogP=args.deltaLogP, downsample=args.downsample, initial_burnin=args.initial_burnin, verbose=args.verbose)
    if args.exponentiate_weight_field:
        if args.verbose:
            print('exponentiating weights')
        weights = np.exp(weights)
    elif args.exponentiate10_weight_field:
        if args.verbose:
            print('exponentiating10 weights')
        weights = 10**weights
else:
    weights = np.ones_like(samples, dtype='float')
weights /= np.sum(weights)

### check prior bounds
truth = (args.prior_min <=samples)*(samples<=args.prior_max)
samples = samples[truth]
weights = weights[truth]

if args.verbose:
    print('partitioning samples into %d subsets'%args.num_subsets)
subsets = utils.partition(samples, weights, num_subsets=args.num_subsets)

sqrtnum_subsets = args.num_subsets**0.5

#-------------------------------------------------

### iterate and compute point estimates for various methods
for foo, b, kwargs, name in [
        (utils.hist, args.hist_bins, {}, 'raw histogram'),
        (utils.chist, args.chist_bins, {'fit_max':args.chist_fit_max}, 'fit cumulative histogram'),
        (utils.kde, args.kde_b, {}, 'kde with reflecting boundaries and b=%.3e'%args.kde_b),
        (utils.max_kde, args.kde_b_range, {'rtol':args.kde_rtol, 'verbose':args.Verbose}, 'kde with reflecting boundaries and maximized bandwidth'),
        (utils.marg_kde, args.kde_b_range, {'rtol':args.kde_rtol, 'dlogl':args.kde_dlogl, 'num_points':args.kde_num_points, 'prior':args.kde_b_prior, 'verbose':args.Verbose}, 'kde with reflecting boundaries and marginalized bandwidth'),
    ]:
    if name in args.skip:
        continue

    print('working on '+name)
    logpost = foo(args.evaluation_point, samples, weights, b, prior_min=args.prior_min, prior_max=args.prior_max, **kwargs)
    logposts = [foo(args.evaluation_point, subsamp, subweight, b, prior_min=args.prior_min, prior_max=args.prior_max, **kwargs) for subsamp, subweight in subsets]

    report = '''\
    logpost = %.6f
    logprior = %.6f
    logpost - logpost = %.6f'''%(logpost, logprior, logpost-logprior)
    if args.Verbose:
        for i, lp in enumerate(logposts):
            report += '''
        subset %02d (%04d samples) logpost - logprior = %.6f'''%(i+1, len(subsets[i][0]), lp-logprior)
    report += '''
    mean(logpost - logprior) = %.6f
    stdv(logpost - logprior) = %.6f'''%(np.mean(logposts)-logprior, np.std(logposts)/sqrtnum_subsets)
    print(report)

### compute distribution for logpost-logprior
print('computing quantiles')
if args.kde_beta_method=='marg':
    logpost, cdf = utils.marg_betakde(
        args.evaluation_point,
        samples,
        weights,
        args.kde_b_range,
        prior_min=args.prior_min,
        prior_max=args.prior_max,
        rtol=args.kde_rtol,
        dlogl=args.kde_dlogl,
        num_points=args.kde_num_points,
        prior=args.kde_b_prior,
        num_quantiles=args.kde_num_quantiles,
        verbose=args.Verbose,
    )
elif args.kde_beta_method=='max':
    logpost, cdf = utils.max_betakde(
        args.evaluation_point,
        samples,
        weights,
        args.kde_b_range,
        prior_min=args.prior_min,
        prior_max=args.prior_max,
        rtol=args.kde_rtol,
        num_quantiles=args.kde_num_quantiles,
        verbose=args.Verbose,
    )
else:
    raise ValueError('--kde-beta-method=%s not understood'%args.kde_beta_method)

### save a representation of the whole process for logpost(prior_min) to disk
path = os.path.join(args.output_dir, 'sddr%s.txt'%args.tag)
if args.verbose:
    print('writing quantiles to: '+path)
with open(path, 'w') as file_obj:
    print >> file_obj, 'logpost-logprior cdf'
    template = '%.6f %.6f'
    for tup in zip(logpost-logprior, cdf):
        print >> file_obj, template%tup

### estimate the expected value of the posterior at this point, and then report the log of that.
### This is essentially what all the other estimators are doing, so we should mimic that
### NOTE: log(E[post]) will "bias" our inference to larger values compared to E[log(post)], so we report both

post = np.exp(logpost)
pdf = np.gradient(cdf, np.gradient(post))
norm = np.trapz(pdf, post)

Elogpost = np.trapz(pdf*logpost, post)/np.trapz(pdf, post)
logEpost = np.log(np.trapz(pdf*post, post)/np.trapz(pdf, post))

### repeat for subsets, without saving to disk
logEposts = []
Elogposts = []
for subsamp, subweight in subsets:
    if args.kde_beta_method=='marg':
        logpost, cdf = utils.marg_betakde(
            args.evaluation_point,
            subsamp,
            subweight,
            args.kde_b_range,
            prior_min=args.prior_min,
            prior_max=args.prior_max,
            rtol=args.kde_rtol,
            dlogl=args.kde_dlogl,
            num_points=args.kde_num_points,
            prior=args.kde_b_prior,
            num_quantiles=args.kde_num_quantiles,
            verbose=args.Verbose,
        )
    elif args.kde_beta_method=='max':
        logpost, cdf = utils.max_betakde(
            args.evaluation_point,
            subsamp,
            subweight,
            args.kde_b_range,
            prior_min=args.prior_min,
            prior_max=args.prior_max,
            rtol=args.kde_rtol,
            num_quantiles=args.kde_num_quantiles,
            verbose=args.Verbose,
        )
    else:
        raise ValueError('--kde-beta-method=%s not understood'%args.kde_beta_method)

    post = np.exp(logpost)
    pdf = np.gradient(cdf, np.gradient(post))
    norm = np.trapz(pdf, post)

    Elogposts.append(np.trapz(pdf*logpost, post)/norm)
    logEposts.append(np.log(np.trapz(pdf*post, post)/norm))

report = '''\
    logE[post] = %.6f
    E[logpost] = %.6f
    logprior = %.6f
    logE[post] - logpost = %.6f
    E[logpost] - logpost = %.6f'''%(logEpost, Elogpost, logprior, logEpost-logprior, Elogpost-logprior)
if args.Verbose:
    for i, (lEp, Elp) in enumerate(zip(logEposts, Elogposts)):
        report += '''
        subset %02d (%04d samples)
            logE[post] - logprior = %.6f
            E[logpost] - logprior = %.6f'''%(i+1, len(subsets[i][0]), lEp-logprior, Elp-logprior, )
report += '''
    mean(logE[post] - logprior) = %.6f
    stdv(logE[post] - logprior) = %.6f
    mean(E[logpost] - logprior) = %.6f
    stdv(E[logpost] - logprior) = %.6f'''%(np.mean(logEposts)-logprior, np.std(logEposts)/sqrtnum_subsets, np.mean(Elogposts)-logprior, np.std(Elogposts)/sqrtnum_subsets)
print(report)
