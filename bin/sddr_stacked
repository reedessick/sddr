#!/usr/bin/env python

__usage__ = "sddr_stacked [--options] samples.hdf5,samples.hdf5,...,samples.hdf5 [samples.hdf5,...,samples.hdf5 samples.hdf5,...,samples.hdf5 ...]"
__doc__ = "an executable that computes the equivalent of sddr, but with stacked events"
__author__ = "reed.essick@ligo.org"

#-------------------------------------------------

import os
import numpy as np

from optparse import OptionParser

### non-standard libraries
from sddr import utils

#-------------------------------------------------

FIELDS = [
    'log10NLTides_A0', ### NOTE: this must always appear as index 0, otherwise marginalization logic will break when estimating the SDDR
    'NLTides_F0',
    'NLTides_N0',
    'NLTides_dlogAdm',
    'NLTides_dFdm',
    'NLTides_dNdm',
]

PRIORS = {
    'log10NLTides_A0': (-10, -5.5),
    'NLTides_F0': (10, 100),
    'NLTides_N0': (-1, +3),
    'NLTides_dlogAdm': (-1, +1),
    'NLTides_dFdm': (-10, +10),
    'NLTides_dNdm': (-1, +1),
}

#-------------------------------------------------

parser = OptionParser(usage=__usage__, description=__doc__)

parser.add_option('-v', '--verbose', default=False, action='store_true')
parser.add_option('-V', '--Verbose', default=False, action='store_true')

parser.add_option('', '--initial-burnin', default=utils.DEFAULT_INITIAL_BURNIN, type='int',
    help='force the code to skip the first --initial-burnin samples, and then proceed with --deltaLogP logic. \
DEFAULT=%d'%utils.DEFAULT_INITIAL_BURNIN)
parser.add_option('', '--deltaLogP', default=utils.DEFAULT_DELTALOGP, type='float',
    help='used when stripping burn-in from hdf5 files')
parser.add_option('', '--downsample', default=utils.DEFAULT_DOWNSAMPLE, type='int',
    help='only retain 1 out of every --downsample samples after stripping burn-in. Used when reading both file types')

parser.add_option('', '--num-points', default=utils.DEFAULT_NUM_POINTS, type='int',
    help='the number of points used in each dimension of the big KDE grid')

parser.add_option('', '--kde-b', default=utils.DEFAULT_B, type='float', 
    help='the bandwidth used with whitened samples when computing the joint KDE')

parser.add_option('', '--exclude', default=[], type='string', action='append',
    help='skip this paramter from within joint KDE')

parser.add_option('-o', '--output-dir', default='.', type='string')
parser.add_option('-t', '--tag', default='', type='string')

parser.add_option('', '--plot-events', default=False, action='store_true')
parser.add_option('', '--plot-result', default=False, action='store_true')

parser.add_option('', '--level', default=[], type='float', action='append',
    help='compute upper limits corresponding to these levels. Can be repeated. \
DEFAULT=[]')

opts, args = parser.parse_args()
assert args, 'please supply at least 1 input argument\n%s'%__usage__
path_sets = [arg.split(',') for arg in args] ### set up lists of files for each event
assert not ('log10NLTides_A0' in opts.exclude), 'can not exclude log10NLTides_A0!\n%s'%__usage__

opts.verbose |= opts.Verbose

if not os.path.exists(opts.output_dir):
    os.makedirs(opts.output_dir)

filetag = "_"+opts.tag if opts.tag else ""

#-------------------------------------------------

### throw out fields I don't care about
fields = [field for field in FIELDS if not (field in opts.exclude)]
Nfields = len(fields)
priors = np.array([PRIORS[field] for field in fields], dtype=float)

#------------------------

### set up kde grid
points = np.array([np.linspace(*prior, num=opts.num_points) for prior in priors])
logkde = np.zeros((opts.num_points,)*Nfields, dtype=float)

if opts.plot_events: ### we may push the bounds of memory available, so only create this if you really want it
    event_logkde = np.zeros((opts.num_points,)*Nfields, dtype=float)

#------------------------

### iterate over events, computing kde's for each and incorporating them into the total
for ind, paths in enumerate(path_sets):
    ### read in samples
    samples = utils.load(paths, field=fields, deltaLogP=opts.deltaLogP, downsample=opts.downsample, initial_burnin=opts.initial_burnin, verbose=opts.verbose)
    samples = np.array([samples[field] for field in fields]) ### just do this for convenience. Get rid of structured arrays because I don't want to think that hard
    _, Nsamp = samples.shape

    ### check prior bounds
    if opts.Verbose:
        print('checking prior bounds')
    truth = np.ones(Nsamp, dtype=bool)
    for ind in xrange(Nfields):
        prior_min, prior_max = priors[ind]
        truth *= (prior_min <=samples[ind])*(samples[ind]<=prior_max)
    Nsamp = np.sum(truth)
    if opts.Verbose:
        print('retaining %d samples'%(Nsamp))
    samples = samples[:,truth]

    ### compute crap
    weights = np.ones(Nsamp, dtype=float)/Nsamp

    ### whiten
    if opts.verbose:
        print('whitening samples')
    (wpoints, wsamples, wpriors), _ = utils.whiten(points, samples, weights, priors)

    ### plot
    if opts.plot_events:
        ### compute KDE
        if opts.verbose:
            print('computing joint %d-dimensional KDE'%Nfields)
        event_logkde = utils.nd_kde(wpoints, wsamples, weights, wpriors, b=opts.kde_b)

        if opts.verbose:
            print('plotting kde for event=%d'%ind)
        fig = plot.kde2fit(points, event_logkde, priors, fields, b=opts.kde_b)
        figname = "%s/sddr_stacked_%d%s.png"%(opts.output_dir, ind, opts.tag)
        if opts.verbose:
            print('saving: '+figname)
        fig.savefig(figname)
        plot.plt.close(fig)
        
#, directory=opts.output_dir, tag=str(ind)+filetag)

    else: # we don't need the second copy of the array, so just add directly
        if opts.verbose:
            print('computing joint %d-dimensional KDE'%Nfields)
        logkde += utils.nd_kde(wpoints, wsamples, weights, wpriors, b=opts.kde_b)

### normalize the total joint kde over many events
logkde -= utils.nd_norm(points, logkde)

#------------------------

### plot kde for sanity checking
if opts.plot_result:
    if opts.verbose:
        print('plotting joint kde')
    fig = plot.kde2fit(points, event_logkde, priors, fields, b=opts.kde_b)
    figname = "%s/sddr_stacked%s.png"%(opts.output_dir, opts.tag)
    if opts.verbose:
        print('saving: '+figname)
    fig.savefig(figname)
    plot.plt.close(fig)

### marginalize logkde -> logkde(logA)
### NOTE: assumes fields[0] = log10NLTides_A0
if opts.verbose:
    print('marginalizing away %s to compute SDDR'%(', '.join(fields[1:])))
for _ in xrange(Nfields-1):
    logkde = utils.nd_marg(points[:-_], logkde)

### write resulting logkde to disk
path = "%s/sddr_stacked-logkde%s.txt"
if opts.verbose:
    print('saving: '+path)
np.savetxt(path, [points[0], logkde], header='log10NLTides_A0', 'logkde')

### compute confidence levels
# compute the cumulative integral via a trapzoidal approximation
integral = np.concatenate((np.array([0]), np.cumsum(0.5*(logkde[1:]+logkde[:-1])*np.diff(points[0])))
for level in opts.level:
    print('log10NLTides_A0 <= %.6e @ %.1f confidence'%(np.interp(level, integral, points[0]), 100*level))

### compute SDDR for logkde(logA)
### NOTE: assumes fields[0] = log10NLTides_A0 and uses some magic numbers based on that
logpost = logkde[0] ### extract the prior value for this
logprior = -np.log(priors[0][1]-priors[0][0])

print('logpost(Amin) = %.6e'%logpost)
print('logprior = %.6e'%logprior)
print('logpost(Amin)-logprior = %.63'%(logpost-logprior))
